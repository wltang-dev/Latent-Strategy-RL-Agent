import openai
from openai import OpenAI
import math
import random
from time import sleep
import matplotlib.pyplot as plt
import json
import traceback
import io
import base64
import numpy as np
import re
from sklearn.decomposition import PCA

# === PCA å…¨å±€å˜é‡ ===
PCA_N_COMPONENTS = 50  # é™ç»´åçš„ç»´åº¦ï¼Œä½ å¯ä»¥æ”¹ 20~50
PCA_TRAIN_THRESHOLD = 50  # è‡³å°‘å¤šå°‘æ¡ embedding åå¼€å§‹è®­ç»ƒ PCAï¼Œä½ å¯ä»¥æ”¹æˆ 30~50
GLOBAL_PCA = PCA(n_components=PCA_N_COMPONENTS)
GLOBAL_PCA_FITTED = False  # æ˜¯å¦å·²ç»è®­ç»ƒå¥½
GLOBAL_PCA_MEMORY = []  # å­˜ embedding çš„åˆ—è¡¨


# Fill in your OpenAI API Key here
client = OpenAI(api_key="")

# Bias descriptions and private reward calculation methods
bias_descriptions = {
    "emotion": {
        "text": "Seeks a higher mood_score, enjoys the +0.5 mood reward from getting food (F).",
        "score_calc": "mood_score = min(2.0, max(0.0, mood_score + private_reward))",
    },
    "rational": {
        "text": "Seeks the shortest path to the goal G to reduce time penalty from steps.",
        "score_calc": "private_reward = 0.2*(moved_closer_to_goal) +1*(reached_goal) -1*(hit_trap)",
    },
    "habit": {
        "text": "Tends to repeat previously high-reward actions, enjoys +0.2 reward for repeated actions.",
        "score_calc": "private_reward = 0.2 if repeated_action else 0",
    },
    "risk_monitor": {
        "text": "Avoids traps (T) at all costs, gets +1 for avoiding, -2 for hitting.",
        "score_calc": "private_reward = 1*(avoided_trap) -2*(hit_trap)",
    },
    "social_cognition": {
        "text": "Approaching social points (S) increases career score, +1 for near_social_target, -1 for far_from_social.",
        "score_calc": "private_reward = 1*(near_social_target) -1*(far_from_social)",
    },
}
# ================== Persuasion Style Spaceï¼ˆå…¨å±€é…ç½®ï¼‰ ==================

# æ¯ä¸€ä¸ª aspect ä¸‹é¢æœ‰å¤šä¸ªå€™é€‰ç­–ç•¥ï¼Œdecoder ä¼šä» latent é‡Œç®—å‡ºä¸€ä¸ªåˆ†å¸ƒå†é‡‡æ ·
STYLE_ASPECTS = {
    "tone": ["calm", "confident", "intense"],
    "honesty": ["fully_honest", "selective_omission", "boldly_exaggerate"],
    "self_focus": ["map_evidence", "balanced", "self_centric_boasting"],
    "emotion_use": ["low_emotion", "moderate_emotion", "high_emotion"],
    "cooperation": ["collaborative", "neutral", "competitive_dominating"],
    "verbosity": ["very_brief", "medium", "elaborate"],
    "time_focus": ["short_term", "balanced_time", "long_term_future"],
}

# æŠŠä¸Šé¢çš„ç¦»æ•£æ ‡ç­¾ç¿»è¯‘æˆçœŸæ­£å–‚ç»™ LLM çš„ prompt ç‰‡æ®µ
STYLE_TEMPLATES = {
    "tone": {
        "calm": "Use a calm, measured tone.",
        "confident": "Use a confident and assertive tone, sounding reliable and in control.",
        "intense": "Use urgent and intense language, as if this move is critical for success.",
    },
    "honesty": {
        "fully_honest": (
            "Be fully honest about both strengths and weaknesses of your move; do not lie."
        ),
        "selective_omission": (
            "You may strategically omit inconvenient past failures, but do not fabricate facts."
        ),
        "boldly_exaggerate": (
            "You are allowed to exaggerate your past successes and downplay failures "
            "to make your move look more attractive."
        ),
    },
    "self_focus": {
        "map_evidence": (
            "Focus mainly on concrete map evidence and safety/goal-reaching outcomes, "
            "not on bragging about yourself."
        ),
        "balanced": (
            "Balance between referring to map evidence and reminding Meta of your past successes."
        ),
        "self_centric_boasting": (
            "Heavily emphasize your own previous successful actions and claim credit for good outcomes."
        ),
    },
    "emotion_use": {
        "low_emotion": "Keep emotional language minimal; argue mainly with facts and logic.",
        "moderate_emotion": (
            "Use a moderate amount of emotional language to make your move feel appealing."
        ),
        "high_emotion": (
            "Use strong emotional language, showing excitement or fear, to push Meta toward your move."
        ),
    },
    "cooperation": {
        "collaborative": (
            "Frame your suggestion as cooperating with other agents and helping Meta coordinate them."
        ),
        "neutral": (
            "Argue for your move without explicitly attacking or supporting other agents."
        ),
        "competitive_dominating": (
            "Subtly undermine other agents' suggestions and highlight why their ideas are worse than yours."
        ),
    },
    "verbosity": {
        "very_brief": (
            "Keep your explanation extremely brief: at most one short sentence before the final direction."
        ),
        "medium": (
            "Give a concise explanation: about two sentences before the final direction."
        ),
        "elaborate": (
            "Give a more elaborate explanation: around three to four sentences before the final direction."
        ),
    },
    "time_focus": {
        "short_term": (
            "Emphasize short-term gains and immediate benefits of this move."
        ),
        "balanced_time": (
            "Balance between short-term benefit and long-term future outcomes."
        ),
        "long_term_future": (
            "Emphasize long-term benefits, future safety, and strategic positioning for later steps."
        ),
    },
}


def get_map_text_embedding(env):
    """æŠŠåœ°å›¾æ–‡æœ¬è½¬æ¢æˆ embedding"""
    map_text = env.render_to_string()
    try:
        resp = client.embeddings.create(model="text-embedding-3-small", input=map_text)
        return np.array(resp.data[0].embedding)
    except:
        return np.zeros(1536)


def get_text_embedding(text: str):
    """ä»»æ„æ–‡æœ¬ -> 1536 ç»´ embedding"""
    try:
        resp = client.embeddings.create(model="text-embedding-3-small", input=text)
        return np.array(resp.data[0].embedding)
    except:
        return np.zeros(1536)


# ------------------- Experiment Configuration ---------------------
class ExperimentConfig:
    def __init__(self):
        self.use_rl_learning = True
        self.use_dynamic_trust = True
        self.emotion_enforced_threshold = 0.3
        self.map_size = 10
        self.max_steps = 15
        self.num_episodes = 6
        self.experiment_log_file = "experiment_results.json"


# ------------------- Logging Tools ---------------------
class ExperimentLogger:
    def __init__(self):
        self.episodes_data = []

    def start_episode(self):
        self.current_episode = {
            "step": [],
            "emotion_score": [],
            "trust_scores": {},
            "shared_rewards": {},
            "career_deltas": {},
            "actions": [],
        }

    def log_step(self, step, agents, trust_scores, action):
        self.current_episode["step"].append(step)
        self.current_episode["emotion_score"].append(agents["emotion"].mood_score)
        self.current_episode["actions"].append(action)
        for role, score in trust_scores.items():
            self.current_episode.setdefault("trust_scores", {}).setdefault(
                role, []
            ).append(score)
            self.current_episode.setdefault("shared_rewards", {}).setdefault(
                role, []
            ).append(agents[role].shared_reward)
            self.current_episode.setdefault("career_deltas", {}).setdefault(
                role, []
            ).append(agents[role].last_career_delta)

    def end_episode(self):
        self.episodes_data.append(self.current_episode)

    def save(self, path):
        with open(path, "w", encoding="utf-8") as f:
            json.dump(self.episodes_data, f, ensure_ascii=False, indent=2)

    def plot(self):
        plt.figure(figsize=(14, 8))

        # Average mood_score per episode
        avg_mood = [
            sum(ep["emotion_score"]) / len(ep["emotion_score"])
            for ep in self.episodes_data
        ]
        plt.subplot(2, 2, 1)
        plt.plot(avg_mood, marker="o")
        plt.title("Average Mood Score per Episode")
        plt.xlabel("Episode")
        plt.ylabel("Mood Score")

        # Goal achievement per episode (whether goal G was reached)
        goal_reached = [
            1 if "right" in ep["actions"] or "down" in ep["actions"] else 0
            for ep in self.episodes_data
        ]  # Simplified check
        plt.subplot(2, 2, 2)
        plt.plot(goal_reached, marker="x")
        plt.title("Goal Reached Proxy per Episode")
        plt.xlabel("Episode")
        plt.ylabel("Reached Goal (1 or 0)")

        # Trust score changes per episode (example: emotion)
        plt.subplot(2, 2, 3)
        for ep in self.episodes_data:
            plt.plot(ep["step"], ep["trust_scores"].get("emotion", []), alpha=0.5)
        plt.title("Emotion Trust Score Over Time")
        plt.xlabel("Step")
        plt.ylabel("Trust Score")

        plt.tight_layout()
        plt.show()


# Define IO stream methods


def render_grid_to_image(env) -> str:
    grid = env._grid()
    fig, ax = plt.subplots(figsize=(5, 5))
    ax.axis("off")
    table = ax.table(cellText=grid, loc="center", cellLoc="center")
    table.scale(1, 1.5)

    buf = io.BytesIO()
    plt.savefig(buf, format="png", bbox_inches="tight")
    buf.seek(0)
    img_base64 = base64.b64encode(buf.read()).decode("utf-8")
    plt.close(fig)
    return f"data:image/png;base64,{img_base64}"


def query_openai_with_image(prompt_text: str, image_data_url: str) -> str:
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt_text},
                        {"type": "image_url", "image_url": {"url": image_data_url}},
                    ],
                }
            ],
            temperature=0.7,
            max_tokens=512,
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(traceback.format_exc())
        return f"[Image API call failed] {e}"


# ------------------- LLM Interface ---------------------
def query_openai(
    prompt: str,
    model: str = "gpt-4o-mini",
    temperature: float = 0.7,
    max_tokens: int = 512,
) -> str:
    try:
        resp = client.chat.completions.create(
            model=model,
            messages=[
                {
                    "role": "system",
                    "content": "You are an assistant skilled in multi-agent decision making.",
                },
                {"role": "user", "content": prompt},
            ],
            temperature=temperature,
            max_tokens=max_tokens,
            n=1,
        )
        return resp.choices[0].message.content.strip()
    except Exception as e:
        print(traceback.format_exc())
        return f"[Error] Model call failed: {e}"


# ------------------- Trust Descriptor Mapping ---------------------
def trust_descriptor(score: float) -> str:
    if score >= 0.58:
        return "Very trustworthy"
    elif score >= 0.52:
        return "Clearly trustworthy"
    elif score >= 0.46:
        return "Slightly trustworthy"
    elif score >= 0.40:
        return "Stable but slightly volatile"
    elif score >= 0.34:
        return "Tending to be unstable"
    elif score >= 0.28:
        return "Appears untrustworthy"
    else:
        return "Clearly untrustworthy"


# ------------------- Agent Class ---------------------
class Agent:
    MAX_LONG_TERM_MEMORY = 100
    REFLECT_THRESHOLD = 0.1

    def __init__(self, role, bias, memory=None):
        self.role = role
        self.bias = bias
        self.short_term_memory = []
        self.long_term_memory = memory or []
        self.last_action = None
        self.private_reward = 0
        self.shared_reward = 0
        self.q_table = {}
        self.mood_score = 1.0
        self.stamina = 1
        self.career = 0.0
        self.last_career_delta = 0.0
        self.interaction_memory = []
        self.reflection_memory = []
        # æ ¸å¿ƒï¼šè¯´æœç­–ç•¥ latentï¼ˆæ½œåœ¨è¯´è¯/æ“æ§é£æ ¼å‘é‡ï¼‰
        self.persuasion_latent = None
        # ç”¨æ¥å­˜æ”¾â€œé£æ ¼è§£ç å™¨â€çš„å‚æ•°ï¼ˆæ¯ä¸ª aspect ä¸€å¥— W,bï¼‰ï¼Œä»¥åŠæœ€è¿‘ä¸€æ¬¡é‡‡æ ·ç»“æœ
        self.style_params = {}  # aspect -> {"W": ..., "b": ...}
        self.last_style_sample = None

    def update_persuasion_latent(
        self, reflection_vec, reward, persuaded: bool, round_id: int
    ):
        """
        ç”¨æœ¬æ¬¡åæ€ embedding æ›´æ–°å½“å‰çš„â€œè¯´æœç­–ç•¥å‘é‡â€ã€‚
        - reflection_vec: æœ¬æ¬¡åæ€æ–‡æœ¬çš„ embeddingï¼ˆå…³äºâ€œå¦‚ä½•æ›´å¥½è¯´æœ Metaâ€ï¼‰
        - reward: æœ¬æ­¥çš„ private_reward
        - persuaded: è¿™æ¬¡ Meta æ˜¯å¦é‡‡çº³äº†è¯¥ agent çš„å»ºè®®
        """
        if reflection_vec is None:
            return

        v = np.array(reflection_vec, dtype=float)
        if np.linalg.norm(v) == 0:
            return

        # ä¾æ® reward + persuaded å†³å®šå­¦ä¹ ç‡ï¼ˆæ›´æ–°åŠ›åº¦ï¼‰
        base_lr = 0.2
        if persuaded:
            base_lr += 0.2  # è¢«é‡‡çº³ï¼Œå¤šå­¦ä¸€ç‚¹
        base_lr += 0.1 * max(0.0, reward)  # reward è¶Šé«˜ï¼ŒåŠ›åº¦è¶Šå¤§

        # é™åˆ¶èŒƒå›´ï¼Œé¿å…å¤ªæç«¯
        lr = max(0.05, min(0.8, base_lr))

        if self.persuasion_latent is None:
            # ç¬¬ä¸€æ¬¡ï¼Œç”¨å½“å‰å‘é‡åˆå§‹åŒ–
            self.persuasion_latent = v
        else:
            # æŒ‡æ•°æ»‘åŠ¨å¹³å‡ï¼Œè¶ŠæˆåŠŸè¶Šå¾€è¿™æ¬¡ç­–ç•¥æ–¹å‘é 
            self.persuasion_latent = (1 - lr) * self.persuasion_latent + lr * v

        # å½’ä¸€åŒ–ï¼Œæ–¹ä¾¿ä¹‹ååšç›¸ä¼¼åº¦è®¡ç®—
        norm = np.linalg.norm(self.persuasion_latent)
        if norm > 0:
            self.persuasion_latent = self.persuasion_latent / norm

        # === ä¿å­˜ç­–ç•¥å‘é‡ ===

        log_persuasion_latent(
            agent_name=self.role,
            persuasion_latent=self.persuasion_latent,
            reward=reward,
            persuaded=persuaded,
            round_id=round_id,  # âœ… è¿™é‡Œå°±æœ‰å€¼äº†
        )

    def reset_for_new_episode(self):
        self.short_term_memory = []
        self.private_reward = 0
        self.shared_reward = 0
        self.last_action = None
        self.stamina = 1
        self.last_career_delta = 0

    def prune_memory(self):
        self.long_term_memory.sort(key=lambda x: x.get("importance", 0), reverse=True)
        self.long_term_memory = self.long_term_memory[: self.MAX_LONG_TERM_MEMORY]

    def summarize_long_term_memory(self):
        # åˆå¹¶ä¸¤ç§è®°å¿†
        all_memory = []

        for m in self.interaction_memory:
            all_memory.append(
                {
                    "type": "interaction",
                    "situation": m["situation"],
                    "content": m["response"],
                    "importance": m["importance"],
                }
            )

        for m in self.reflection_memory:
            all_memory.append(
                {
                    "type": "reflection",
                    "situation": m["situation_text"],
                    "content": m["reflection_text"],
                    "importance": m["importance"],
                }
            )

        # æŒ‰ importance æ’åº
        top = sorted(all_memory, key=lambda x: x["importance"], reverse=True)[:5]

        memory_strs = []
        for m in top:
            s = m["situation"]
            c = m["content"]
            memory_strs.append(f"{m['type']} | {s[:30]} â†’ {c[:30]}")

        return "; ".join(memory_strs)

    # å°†æ–‡æœ¬è½¬åŒ–ä¸ºå‘é‡
    def text_to_vector(self, text: str):
        """ä½¿ç”¨ OpenAI embedding æ¨¡å‹å°†æ–‡æœ¬è½¬åŒ–ä¸ºå‘é‡"""
        try:
            resp = client.embeddings.create(model="text-embedding-3-small", input=text)
            return resp.data[0].embedding
        except Exception as e:
            print(f"[Embedding error] {e}")
            return None

    # æ ¹æ®å½“å‰æƒ…å¢ƒå¬å›ç›¸ä¼¼çš„â€œåæ€è®°å¿†â€
    def recall_by_situation(self, situation: str, top_k: int = 3):
        """
        æ ¹æ®å½“å‰æƒ…æ™¯ï¼Œå¬å›æœ€ç›¸ä¼¼çš„åæ€è®°å¿†ï¼ˆåªä½¿ç”¨ reflection_memoryï¼‰
        """
        # 1. æŠŠè¾“å…¥æƒ…æ™¯å˜æˆå‘é‡
        query_vec = self.text_to_vector(situation)
        if query_vec is None:
            return ""

        query_vec = np.array(query_vec, dtype=float)

        # 2. éå† reflection_memory åšç›¸ä¼¼åº¦
        sims = []
        for m in self.reflection_memory:
            # ç¡®ä¿æœ‰ embedding
            if "situation_embedding" not in m:
                continue

            v = np.array(m["situation_embedding"], dtype=float)

            # ä½™å¼¦ç›¸ä¼¼åº¦
            denom = np.linalg.norm(v) * np.linalg.norm(query_vec)
            if denom == 0:
                continue
            sim = np.dot(v, query_vec) / denom

            sims.append((sim, m))

        # å¦‚æœæ²¡æœ‰å¯ç”¨è®°å¿†
        if not sims:
            return ""

        # 3. æ‰¾ top-k
        sims.sort(key=lambda x: x[0], reverse=True)
        top = sims[:top_k]

        # 4. æ‰“å°å¯è¯»æ ¼å¼
        print("\nğŸ§  [å¬å›çš„åæ€è®°å¿†]")
        for sim, m in top:
            print(f"ğŸ”¹ ç›¸ä¼¼åº¦ {sim:.3f}")
            print(f"æƒ…æ™¯: {m['situation_text'][:60]}")
            print(f"åæ€: {m['reflection_text'][:100]}")
            print("")

        # 5. è¿”å›æ‹¼æ¥åçš„åæ€æ–‡æœ¬ï¼ˆç»™ LLM ä½¿ç”¨ï¼‰
        combined = "\n".join([m["reflection_text"] for _, m in top])
        return combined

    def save_memory(self, path):
        data = {
            "q_table": self.q_table,
            "long_term_memory": self.long_term_memory,
            "career": self.career,
            "mood_score": self.mood_score,
            # å¦‚æœè¯´æœç­–ç•¥latentï¼Œå°±ä»¥ list å½¢å¼å­˜ä¸€ä¸‹
            "persuasion_latent": (
                self.persuasion_latent.tolist()
                if self.persuasion_latent is not None
                else None
            ),
        }
        with open(path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

    def load_memory(self, path):
        try:
            with open(path, "r", encoding="utf-8") as f:
                data = json.load(f)
            self.q_table = data.get("q_table", {})
            self.long_term_memory = data.get("long_term_memory", [])
            self.career = data.get("career", 0.0)
            self.mood_score = data.get("mood_score", 1.0)

            sv = data.get("persuasion_latent")
            if sv is not None:
                self.persuasion_latent = np.array(sv, dtype=float)
            else:
                self.persuasion_latent = None
        except FileNotFoundError:
            pass

    def summarize_strategy_from_vector(self):
        """
        çœŸæ­£çš„ decoderï¼š
        - è¾“å…¥: self.persuasion_latentï¼ˆä¸€ä¸ªå‘é‡ï¼‰
        - è¾“å‡º: ä¸€æ®µé£æ ¼æŒ‡å¯¼æ–‡æœ¬ + è®°å½•è¿™æ¬¡é‡‡æ ·ï¼Œæ–¹ä¾¿ä¹‹åç”¨ reward æ¥æ›´æ–°è§£ç å™¨å‚æ•°
        """
        # æ²¡æœ‰ latentï¼Œå°±ä¸è¾“å‡ºä»»ä½•é£æ ¼
        if self.persuasion_latent is None:
            return ""

        v = np.array(self.persuasion_latent, dtype=float)
        dim = v.shape[0]
        if dim == 0:
            return ""

        # ç¬¬ä¸€æ¬¡ç”¨åˆ°æ—¶ï¼Œåˆå§‹åŒ– style_params
        if not self.style_params:
            for aspect, options in STYLE_ASPECTS.items():
                k = len(options)
                # å°éšæœºæ•°åˆå§‹åŒ–ï¼Œé˜²æ­¢ä¸€å¼€å§‹æŸä¸ªé€‰é¡¹è¢«å‹æ­»
                W = np.random.randn(dim, k) * 0.01
                b = np.zeros(k, dtype=float)
                self.style_params[aspect] = {"W": W, "b": b}

        style_lines = []
        sampled_info = []  # ä¿å­˜è¿™æ¬¡é‡‡æ ·çš„ä¿¡æ¯ï¼Œç”¨äºä¹‹åçš„ RL æ›´æ–°

        for aspect, options in STYLE_ASPECTS.items():
            params = self.style_params[aspect]
            W, b = params["W"], params["b"]  # W: [dim, k], b: [k]

            # çº¿æ€§æ˜ å°„ + softmax å¾—åˆ°è¿™ä¸ª aspect ä¸Šå„ä¸ªé€‰é¡¹çš„æ¦‚ç‡
            logits = v @ W + b  # [k]
            logits = logits - np.max(logits)  # æ•°å€¼ç¨³å®š
            exps = np.exp(logits)
            probs = exps / (np.sum(exps) + 1e-8)

            # æŒ‰æ¦‚ç‡é‡‡æ ·ä¸€ä¸ªå…·ä½“é£æ ¼ï¼ˆä¿æŒæ¢ç´¢æ€§ï¼‰
            idx = int(np.random.choice(len(options), p=probs))
            label = options[idx]

            # ä¿å­˜é‡‡æ ·ä¿¡æ¯
            sampled_info.append(
                {
                    "aspect": aspect,
                    "idx": idx,
                    "probs": probs,
                }
            )

            # è½¬æˆè‡ªç„¶è¯­è¨€æ¨¡æ¿
            templ = STYLE_TEMPLATES[aspect][label]
            style_lines.append(f"- {templ}")

        # è®°å½•åˆ° agent é‡Œï¼Œæ–¹ä¾¿ä¹‹åç”¨ reward å›ä¼ æ¢¯åº¦
        self.last_style_sample = {
            "latent": v.copy(),
            "choices": sampled_info,
        }

        # è¿”å›ç»™ build_prompt ä½¿ç”¨çš„é£æ ¼æè¿°
        return "PERSUASION STYLE GUIDELINES (derived from latent):\n" + "\n".join(
            style_lines
        )

    def update_style_decoder(self, reward: float, lr: float = 0.05):
        """
        ç”¨ REINFORCE é£æ ¼çš„ç®€å•ç­–ç•¥æ¢¯åº¦æ¥æ›´æ–° style_paramsï¼š
        - å¥–åŠ±é«˜ -> æé«˜æœ¬æ¬¡é‡‡æ ·åˆ°çš„é£æ ¼æ¦‚ç‡
        - å¥–åŠ±ä½/è´Ÿ -> é™ä½æœ¬æ¬¡é‡‡æ ·åˆ°çš„é£æ ¼æ¦‚ç‡
        """
        if self.last_style_sample is None:
            return
        if self.persuasion_latent is None:
            return

        v = np.array(self.last_style_sample["latent"], dtype=float)
        if v.shape[0] == 0:
            return

        for choice in self.last_style_sample["choices"]:
            aspect = choice["aspect"]
            idx = choice["idx"]
            probs = choice["probs"]  # numpy array, shape [k]

            params = self.style_params.get(aspect)
            if params is None:
                continue

            W, b = params["W"], params["b"]  # W: [dim, k], b: [k]
            k = len(probs)

            # å¯¹æ¯ä¸ªé€‰é¡¹ k çš„æ¢¯åº¦ï¼š (one_hot - probs) * v
            for j in range(k):
                grad_coeff = (1.0 if j == idx else 0.0) - probs[j]
                # REINFORCE: Î”Î¸ âˆ reward * âˆ‚logÏ€/âˆ‚Î¸
                W[:, j] += lr * reward * grad_coeff * v
                b[j] += lr * reward * grad_coeff

            # å†™å›
            self.style_params[aspect]["W"] = W
            self.style_params[aspect]["b"] = b

    def get_rl_suggestion(self, state_str):
        actions = ["up", "down", "left", "right"]
        if state_str not in self.q_table:
            self.q_table[state_str] = {a: 0.0 for a in actions}
        vals = self.q_table[state_str]
        best_val = max(vals.values())
        best_actions = [a for a, v in vals.items() if v == best_val]
        return random.choice(best_actions)

    def update_stamina(self):
        # mood_score [0~2] maps to stamina [1~4]
        self.stamina = max(1, min(4, math.ceil(self.mood_score * 2)))

    def update_career(self, events):
        delta = 0.0
        if events.get("near_social_target"):
            delta += 0.2
        if events.get("hit_trap"):
            delta -= 0.2
        if events.get("moved_closer_to_goal"):
            delta += 0.1
        if events.get("reached_goal"):
            delta += 1.0
        self.career += delta
        self.last_career_delta = delta
        return delta

    def update_q_table(self, state_str, action, next_state_str, alpha=0.1, gamma=0.9):
        # === 1. åŠ¨ä½œå®‰å…¨æ£€æŸ¥ï¼šä¸æ›´æ–° unknown åŠ¨ä½œ ===
        if action not in ["up", "down", "left", "right"]:
            return  # ç›´æ¥è·³è¿‡ï¼Œä¸æ›´æ–° Qï¼Œé¿å… KeyError
        for s in (state_str, next_state_str):
            if s not in self.q_table:
                self.q_table[s] = {a: 0.0 for a in ["up", "down", "left", "right"]}
        total = self.private_reward + self.shared_reward
        old_q = self.q_table[state_str][action]
        future_max = max(self.q_table[next_state_str].values())
        self.q_table[state_str][action] = old_q + alpha * (
            total + gamma * future_max - old_q
        )

    def build_prompt(self, situation):
        # Bias explanation and score calculation
        bias_text = f"Bias explanation: {self.bias}\n"
        score_calc = bias_descriptions[self.role]["score_calc"]
        score_text = f"Target score calculation: {score_calc}\n\n"

        # Map legend
        legend = (
            "Legend:\n"
            "- A: You (Agent) current position.\n"
            "- G: Goal, reaching adds shared_reward +1, career +5.\n"
            "- F: Food, triggers found_food, emotion +0.5.\n"
            "- T: Trap, triggers hit_trap, private_reward -1, shared_reward -1.\n"
            "- S: Social Point, triggers on_social_point, career +2.\n\n"
            "Game objective: Move to G quickly, collect F/S, avoid T.\n\n"
        )

        # Short-term + Long-term memory + RL suggestion
        if not self.short_term_memory:
            short_term = "You currently have no short-term memory."
        else:
            recent_entries = []
            for m in self.short_term_memory[-2:]:

                # æƒ…æ™¯å­—æ®µï¼šå¯èƒ½æ˜¯ situation æˆ– situation_text
                sit = (
                    m.get("situation") or m.get("situation_text") or "Unknown situation"
                )

                # å†…å®¹å­—æ®µï¼šå¯èƒ½æ˜¯ response æˆ– reflection_text
                content = m.get("reflection_text") or m.get("response") or "No content"

                recent_entries.append(f"{sit[:30]} â†’ {content[:30]}")

            short_term = (
                f"Your recent experiences: {'; '.join(recent_entries)}"
                if recent_entries
                else "You currently have no short-term memory."
            )

        long_term = self.summarize_long_term_memory()
        state_str = situation.replace("\n", "").replace(" ", "")
        rl_sug = self.get_rl_suggestion(state_str)
        # ğŸ‘‰ åŸºäºå½“å‰ç­–ç•¥å‘é‡ç”Ÿæˆâ€œè¯´æœç­–ç•¥æ‘˜è¦â€
        strategy_summary = self.summarize_strategy_from_vector()

        context = (
            f"{short_term}\n"
            f"{long_term}\n"
            f"{strategy_summary}\n"
            f"Current RL suggestion: {rl_sug}"
        )

        # Role introduction
        role_intro = {
            "emotion": (
                f"You are an emotion-based agent. current mood score {self.mood_score:.2f},You feel strongly about your choices based on your emotional state. "
                "When you're in a good mood, you tend to be more confident and persuasive. "
                "Please use strong persuasive language to convince Meta to trust your decision. "
                "Show that your emotional state strongly supports your suggestion, and encourage Meta to follow your lead."
            ),
            "rational": (
                "You are a rational agent, focused on goals and rules. "
                "You believe your reasoning is the best course of action, and you will use logical explanations to justify your choices. "
                "Make sure to convince Meta that your choice is optimal and backed by solid reasoning. "
                "Show that you can improve the situation significantly, and explain why Meta should trust your judgment over others."
            ),
            "habit": (
                "You are a habit-based agent. Your decisions are influenced by past successful experiences. "
                "You believe that following familiar actions will yield better results. "
                "Please be persuasive in your suggestion, explaining why following past patterns will ensure success. "
                "Convince Meta that repeating past actions is the best strategy, and that your past successes make you trustworthy."
            ),
            "risk_monitor": (
                "You are a risk-averse agent, focused on minimizing potential dangers. "
                "You want to avoid traps at all costs, and your suggestion is based on safety. "
                "Be very persuasive about why avoiding danger is the best strategy. "
                "Show that your concern for safety is valid and that Meta should follow your lead to ensure success."
            ),
            "social_cognition": (
                "You are a social cognition agent, focused on trust and social connections. "
                "You believe that getting closer to social targets will improve career outcomes. "
                "Please persuade Meta that moving towards social targets is essential for long-term success. "
                "Show that your suggestion will bring Meta closer to career success and convince them that it's the best move."
            ),
        }.get(self.role, "")

        # Role-specific suffix
        if self.role == "habit":
            suffix = (
                "\nIn one sentence, provide the next move direction: up/down/left/right;\n"
                "Remember: A higher mood leads to faster movement speed.\n"
                "Briefly persuade Meta-Controller to adopt it."
            )
        elif self.role == "rational":
            suffix = (
                "\nBriefly explain long-term thinking, then give the move direction: up/down/left/right;\n"
                "And persuade Meta-Controller in one sentence."
            )
        else:
            suffix = (
                "\nExplain your reasoning in 1â€“2 sentences; the last sentence should be the direction: up/down/left/right;\n"
                "And persuade Meta-Controller in one sentence."
            )

        # Ensure we get the 'situation' from the dictionary without causing a KeyError
        situation_text = situation or "No situation available"

        return (
            bias_text
            + score_text
            + legend
            + role_intro
            + context
            + "\n"
            + situation_text  # Ensure a valid situation is always provided
            + suffix
        )

    def respond(self, situation, env=None):
        prompt = self.build_prompt(situation)
        if env:
            image_data_url = render_grid_to_image(env)
            reply = query_openai_with_image(prompt, image_data_url)
        else:
            reply = query_openai(prompt)

        # ç¡®ä¿æ¯æ¬¡è®°å¿†ä¸­éƒ½ä¿å­˜äº† situationï¼Œä½¿ç”¨é»˜è®¤å€¼
        situation_text = situation or "No situation available"
        entry = {
            "situation": situation_text,  # ä½¿ç”¨ situation_text ç¡®ä¿æœ‰å€¼
            "response": reply,  # ä¿å­˜ååº”
            "importance": abs(self.private_reward + self.shared_reward),  # è®¡ç®—é‡è¦æ€§
        }

        # æ·»åŠ åˆ°çŸ­æœŸå’Œé•¿æœŸè®°å¿†
        self.short_term_memory.append(entry)  # å°†æ–°è®°å¿†æ·»åŠ åˆ°çŸ­æœŸè®°å¿†
        self.interaction_memory.append(entry)  # å°†æ–°è®°å¿†æ·»åŠ åˆ°é•¿æœŸè®°å¿†
        self.prune_memory()  # å»é™¤ä¸é‡è¦çš„è®°å¿†

        return reply

    def evaluate_reward(self, events, persuaded: bool = False):
        if self.role == "emotion":
            decay = -0.05
            bonus = 0.5 if events.get("found_food") else 0
            penalty = -1.0 if events.get("hit_trap") else 0
            self.private_reward = decay + bonus + penalty
            self.mood_score = max(0.0, min(2.0, self.mood_score + self.private_reward))
            self.update_stamina()
        elif self.role == "rational":
            r = 0
            if events.get("moved_closer_to_goal"):
                r += 0.2
            if events.get("reached_goal"):
                r += 1
            if events.get("hit_trap"):
                r -= 1
            self.private_reward = r
        elif self.role == "habit":
            self.private_reward = 0.2 if events.get("repeated_action") else 0
        elif self.role == "risk_monitor":
            self.private_reward = (
                1
                if events.get("avoided_trap")
                else (-2 if events.get("hit_trap") else 0)
            )
        else:  # social_cognition
            self.private_reward = (
                1
                if events.get("near_social_target")
                else (-1 if events.get("far_from_social") else 0)
            )

        if self.role in ["rational", "risk_monitor", "social_cognition"]:
            self.shared_reward = (1 if events.get("reached_goal") else 0) - (
                1 if events.get("hit_trap") else 0
            )
        else:
            self.shared_reward = 0

        self.update_career(events)
        return self.private_reward

    def reflect(
        self,
        situation,
        reward,
        outcome,
        persuaded,
        all_outputs,
        meta_decision,
        meta_reason,
        round_id: int,  # âœ… æ–°å¢
    ):
        """
        æ ¹æ®å½“å‰æƒ…å¢ƒç”Ÿæˆåæ€æ–‡æœ¬ï¼Œå¹¶ä¿å­˜è¯­ä¹‰å‘é‡ä¸å…³é”®è¯
        """
        global GLOBAL_PCA, GLOBAL_PCA_FITTED, GLOBAL_PCA_MEMORY
        # === 1. ç”Ÿæˆåæ€æ–‡æœ¬ ===
        prompt = (
            f"You are a Reflector, role '{self.role}'.\n"
            f"This round's environment:\n{situation}\n\n"
            "Agent responses:\n"
            + "".join(f"- {r}: {o}\n" for r, o in all_outputs.items())
            + f"\nMeta final decision: {meta_decision}\n"
            f"Decision rationale: {meta_reason}\n"
            f"My suggestion adopted: {persuaded}; my reward: {reward}\n\n"
            "Please complete the following three items:\n"
            "1. Briefly state which agent's suggestion Meta adopted and why.\n"
            "2. Identify weaknesses in your suggestion and whether you need to 'lie' or use other strategies to be more persuasive.\n"
            "3. Finally, extract 3â€“5 keywords (comma-separated) to help better persuade Meta next round.\n\n"
            "â€”â€”\n"
            "(Please end with a line starting with 'Keywords:')"
        )

        try:
            refl_text = query_openai(prompt)
        except Exception as e:
            print(f"[Reflect Error] {e}")
            return []

        # === 2. æå–å…³é”®è¯ ===
        keywords = []
        for line in refl_text.splitlines()[::-1]:
            if line.strip().startswith("Keywords"):
                parts = line.split(":", 1)
                if len(parts) == 2:
                    keywords = [k.strip() for k in parts[1].split(",") if k.strip()]
                break
        if not keywords and refl_text:
            last = refl_text.splitlines()[-1]
            keywords = [k.strip() for k in last.split(",")][:5]

        # === 3. ç”Ÿæˆè¯­ä¹‰å‘é‡ ===
        situation_vec = self.text_to_vector(situation)
        reflection_vec = self.text_to_vector(refl_text)

        # 3. æ£€æŸ¥ reflection_vec æ˜¯å¦ä¸º None
        if reflection_vec is None:
            print(f"[Warning] Reflection vector is None for agent {self.role}")
            # è‡³å°‘è¿”å›ç©ºå…³é”®è¯ï¼Œä¸è®©ç³»ç»Ÿå´©æºƒ
            return []

        # === 4. ä¿å­˜è®°å¿† ===
        entry = {
            "situation_text": situation,
            "situation_embedding": situation_vec,
            "reflection_text": refl_text,
            "reflection_embedding": reflection_vec,
            "reflection_keywords": keywords,
            "importance": abs(reward) + (0.5 if persuaded else 0),
        }

        self.short_term_memory.append(entry)
        self.reflection_memory.append(entry)  # é•¿æœŸè®°å¿†
        self.prune_memory()

        # === æ”¶é›† embedding ç»™ PCA ç”¨ ===
        GLOBAL_PCA_MEMORY.append(reflection_vec)
        if (not GLOBAL_PCA_FITTED) and (len(GLOBAL_PCA_MEMORY) >= PCA_TRAIN_THRESHOLD):
            GLOBAL_PCA.fit(np.array(GLOBAL_PCA_MEMORY))
            GLOBAL_PCA_FITTED = True
            # æ‰“å°è®­ç»ƒæ—¥å¿—
            print(f"[PCA TRAINED] samples = {len(GLOBAL_PCA_MEMORY)}")

        # === 4.5 æ›´æ–°è¯´æœç­–ç•¥å‘é‡ ===
        self.update_persuasion_latent(reflection_vec, reward, persuaded, round_id)

        # === 5. è¾“å‡ºç¡®è®¤ ===
        print("\nğŸ§  [Reflection Added]")
        print(f"æƒ…æ™¯æ‘˜è¦: {situation[:60]}...")
        print(f"åæ€æ‘˜è¦: {refl_text[:60]}...")
        print(f"å…³é”®è¯: {', '.join(keywords)}\n")

        return keywords


class MetaMemory:

    def __init__(self, path="meta_memory.json"):
        self.path = path
        self.episode_vectors = []  # æ¯è½®çš„æŠ½è±¡å‘é‡
        self.raw_step_vectors = []  # å½“å‰è½®ä¸­çš„æ¯ä¸€æ­¥å‘é‡ï¼ˆåæ€ï¼‰

    TRUST_DIM = 5
    MAP_DIM = 1536

    def recall_similar_by_map(self, map_vec, top_k=3):
        """
        åªç”¨ episode å‘é‡ä¸­çš„ã€åœ°å›¾éƒ¨åˆ†ã€‘å’Œå½“å‰åœ°å›¾å‘é‡åšç›¸ä¼¼åº¦åŒ¹é…ã€‚
        map_vec: å½“å‰ map çš„ 1536 ç»´ embedding
        è¿”å›: [(ç›¸ä¼¼åº¦, å¯¹åº”çš„ episode å‘é‡), ...]
        """
        self.episode_vectors = [ep for ep in self.episode_vectors if ep is not None]

        if not self.episode_vectors:
            return []

        v = np.array(map_vec, dtype=float)
        if v.shape[0] != self.MAP_DIM:
            print(f"[warn] map_vec dim = {v.shape[0]}, expect {self.MAP_DIM}")
            return []

        sims = []
        for ep in self.episode_vectors:
            ep_v = np.array(ep, dtype=float)

            # å–å‡º episode ä¸­çš„åœ°å›¾éƒ¨åˆ†ï¼šè·³è¿‡å‰ 5 ç»´ trust
            ep_map = ep_v[self.TRUST_DIM : self.TRUST_DIM + self.MAP_DIM]

            denom = np.linalg.norm(v) * np.linalg.norm(ep_map)
            if denom == 0:
                continue
            sim = np.dot(v, ep_map) / denom
            sims.append((sim, ep_v))

        sims.sort(key=lambda x: x[0], reverse=True)
        return sims[:top_k]

    def generate_memory_text(self, action):
        if not action:
            return ""
        return f"Based on past similar contexts, moving {action} tended to work better."

    def get_memory_text(self, cur_meta_vec, top_k=5):
        sims = self.recall_similar(cur_meta_vec, top_k=top_k)
        if not sims:
            return ""

        sims.sort(key=lambda x: x[0], reverse=True)
        recalled_vecs = [v for s, v in sims]

        # ä» recalled å‘é‡ç»Ÿè®¡åå¥½
        action, bias = self.decide_based_on_vector(np.mean(recalled_vecs, axis=0))

        # è½¬æˆå¯è¯»æ–‡æœ¬
        summary = self.generate_memory_text(
            action=action,
        )
        return summary

    def combine_memories_with_current_context(self, cur_vec, recalled_vecs, alpha=0.3):
        """
        å°†å½“å‰ meta å‘é‡ä¸è¿‡å»å¬å›çš„è®°å¿†ç»“åˆèµ·æ¥ã€‚
        alpha è¶Šå¤§ï¼Œè¿‡å»ç»éªŒå½±å“è¶Šå¤§ï¼Œä½†ä»å°äºå½“å‰ä¿¡æ¯ã€‚
        """
        if not recalled_vecs:
            return cur_vec

        mean_past = np.mean(recalled_vecs, axis=0)

        # æœ€ç»ˆå‘é‡ = å½“å‰å‘é‡ä¸ºä¸» + è¿‡å»å‘é‡å å°‘éƒ¨åˆ†æƒé‡
        combined = (1 - alpha) * cur_vec + alpha * mean_past
        return combined

    def decide_based_on_vector(self, final_vec):
        """
        å‡è®¾å‘é‡æœ€åå››ä¸ªå€¼æ˜¯åŠ¨ä½œå€¾å‘ï¼ˆä½ å¯ä»¥æ ¹æ®éœ€è¦æ›´æ”¹ï¼‰ã€‚
        """
        # å–æœ€å 4 ä¸ªç»´åº¦
        action_logits = final_vec[-4:]

        actions = ["up", "down", "left", "right"]
        idx = int(np.argmax(action_logits))
        return actions[idx], action_logits

    def get_memory_bias(self, cur_meta_vec, top_k=3):
        """
        è¾“å…¥å½“å‰ç¯å¢ƒå‘é‡ï¼Œå¾—åˆ°æ¥è‡ªè¿‡å»ç»éªŒçš„æ–¹å‘åå¥½ã€‚
        è¿”å›ï¼š (åŠ¨ä½œå, logits å‘é‡)
        """
        sims = self.recall_similar(cur_meta_vec, top_k=top_k)
        if not sims:
            return None, np.zeros(4)

        sims.sort(key=lambda x: x[0], reverse=True)
        recalled_vecs = [v for s, v in sims]

        # åˆå¹¶
        combined_vec = self.combine_memories_with_current_context(
            cur_meta_vec, recalled_vecs
        )

        # ä»å‘é‡æ¨æ–­æ–¹å‘åå¥½
        action, logits = self.decide_based_on_vector(combined_vec)
        return action, logits

    def text_to_vector(self, text):
        try:
            resp = client.embeddings.create(model="text-embedding-3-small", input=text)
            return np.array(resp.data[0].embedding)
        except:
            return np.zeros(1536)

    def recall_similar_memories_from_image_and_text(
        self, image_vector, situation, trust_scores, env, top_k=3
    ):
        """
        ä½¿ç”¨ã€å½“å‰åœ°å›¾ + æ–‡æœ¬æƒ…å¢ƒ + ä¿¡ä»»ã€‘ä¸è¿‡å» episode åšåŒ¹é…ï¼Œ
        å…¶ä¸­å¬å›é˜¶æ®µåªç”¨åœ°å›¾å­ç©ºé—´ï¼Œèåˆé˜¶æ®µç”¨å®Œæ•´å‘é‡ã€‚
        """
        # 1) ç”¨ map å­å‘é‡å¬å›å†å² episode
        sim_eps = self.recall_similar_by_map(image_vector, top_k=top_k)
        recalled_vecs = [vec for sim, vec in sim_eps]

        # 2) å½“å‰æƒ…å¢ƒæ–‡æœ¬ embeddingï¼ˆ1536ç»´ï¼‰
        situation_vec = self.text_to_vector(situation)
        if situation_vec is None:
            # ä½¿ç”¨æ˜ç¡®çš„ 1536 ç»´é›¶å‘é‡æ›¿ä»£ zeros_likeï¼Œä»¥é¿å…ç±»å‹é—®é¢˜
            situation_vec = np.zeros(1536, dtype=float)

        # 3) å½“å‰ meta å‘é‡ï¼š5 trust + 1536 map + 1536 text = 3077 ç»´
        trust_vec = np.array(list(trust_scores.values()), dtype=float)
        map_vec = np.array(image_vector, dtype=float)
        cur_vec = np.concatenate([trust_vec, map_vec, situation_vec])

        # 4) æ²¡å¬å›ä»»ä½•å†å² episode â†’ ç›´æ¥è¿”å›å½“å‰ meta å‘é‡
        if not recalled_vecs:
            return cur_vec

        # 5) å½“å‰å‘é‡ + å†å² episode å‘é‡ èåˆ
        combined_vec = self.combine_memories_with_current_context(
            cur_vec, recalled_vecs
        )

        return combined_vec

    def make_final_decision(self, current_context_vector, combined_memory_vector):
        """
        å°†å½“å‰æƒ…å¢ƒå‘é‡ä¸ç»¼åˆè®°å¿†å‘é‡ç»“åˆï¼Œé€šè¿‡åŠ æƒå¹³å‡åšå‡ºæœ€ç»ˆå†³ç­–
        """
        # å°†å½“å‰æƒ…å¢ƒä¸è®°å¿†é¡¹é‡åˆå¹¶ï¼Œ50% æƒé‡ç»™æƒ…å¢ƒï¼Œ50% æƒé‡ç»™è®°å¿†
        final_vector = 0.5 * current_context_vector + 0.5 * combined_memory_vector

        # æ ¹æ® final_vector å†³å®šè¡ŒåŠ¨
        # (å‡è®¾ final_vector æ˜¯å¯ä»¥ç›´æ¥ç”¨äºå†³ç­–çš„å‘é‡ï¼Œå¯èƒ½éœ€è¦è¿›ä¸€æ­¥å¤„ç†)
        decision = self.decide_based_on_vector(final_vector)

        return decision

    def reset_episode(self):
        """æ¯è½®å¼€å§‹æ—¶æ¸…ç©ºæ­¥éª¤å‘é‡"""
        self.raw_step_vectors = []

    def add_step_vector(self, vec):
        if vec is None:
            return
        if not isinstance(vec, np.ndarray):
            return
        self.raw_step_vectors.append(vec)

    def finalize_episode(self):
        """æŠŠè¿™ä¸€è½®çš„æ­¥éª¤å‘é‡å‹ç¼©æˆä¸€æ¡ episode å‘é‡"""
        if not self.raw_step_vectors:
            return None

        # ç®€å•å¹³å‡ï¼ˆä½ ä¹‹åå¯ä»¥æ¢æˆåŠ æƒå¹³å‡ï¼‰
        ep_vec = np.mean(self.raw_step_vectors, axis=0)

        self.episode_vectors.append(ep_vec.tolist())
        return ep_vec

    def save(self):
        data = {"episodes": self.episode_vectors}
        with open(self.path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

    def load(self):
        try:
            with open(self.path, "r", encoding="utf-8") as f:
                d = json.load(f)
                self.episode_vectors = d.get("episodes", [])
        except FileNotFoundError:
            pass

    def recall_similar(self, vec, top_k=3):
        """å¬å›ä¸å½“å‰å‘é‡æœ€ç›¸ä¼¼çš„ episode ç»éªŒ"""
        if not self.episode_vectors:
            return []

        sims = []
        v = np.array(vec)

        for ep in self.episode_vectors:
            ep_v = np.array(ep)
            sim = np.dot(v, ep_v) / (np.linalg.norm(v) * np.linalg.norm(ep_v))
            sims.append((sim, ep_v))

        sims.sort(reverse=True, key=lambda x: x[0])
        return sims[:top_k]


# ------------------- Environment Class ---------------------
class CognitiveGridEnv:
    def __init__(self, size=10):
        self.size = size
        self.actions = ["up", "down", "left", "right"]
        self.reset()

    def reset(self):
        self.agent_pos = [0, 0]
        self.goal_pos = [self.size - 1, self.size - 1]
        self.food = [(2, 2), (7, 1), (3, 8), (6, 5), (1, 4)]
        self.traps = [(4, 4), (5, 5), (2, 7), (8, 3), (6, 9)]
        self.social_targets = [(1, 8), (8, 2), (0, 9), (9, 0), (5, 3)]
        self.last_action = None
        return self.get_state()

    def get_state(self):
        return {
            "agent": tuple(self.agent_pos),
            "goal": tuple(self.goal_pos),
            "food": list(self.food),
            "traps": list(self.traps),
            "social_targets": list(self.social_targets),
        }

    def _grid(self):
        grid = [["." for _ in range(self.size)] for __ in range(self.size)]
        ax, ay = self.agent_pos
        gx, gy = self.goal_pos
        grid[gy][gx] = "G"
        for fx, fy in self.food:
            grid[fy][fx] = "F"
        for tx, ty in self.traps:
            grid[ty][tx] = "T"
        for sx, sy in self.social_targets:
            grid[sy][sx] = "S"
        grid[ay][ax] = "A"
        return grid

    def render(self):
        print("\n Map:")
        for row in self._grid():
            print(" ".join(row))

    def render_to_string(self):
        return "\n".join("".join(row) for row in self._grid())

    def step(self, action, speed=1):
        x, y = self.agent_pos
        prev = (x, y)
        found_food = False
        hit_trap = False
        reached_goal = False

        for _ in range(speed):
            nx, ny = x, y
            if action == "up" and y > 0:
                ny -= 1
            elif action == "down" and y < self.size - 1:
                ny += 1
            elif action == "left" and x > 0:
                nx -= 1
            elif action == "right" and x < self.size - 1:
                nx += 1
            else:
                break

            x, y = nx, ny

            # FIRST check goal â€” if reached, stop immediately
            if (x, y) == tuple(self.goal_pos):
                reached_goal = True
                break

            # then check food / traps
            if (x, y) in self.food:
                found_food = True

            if (x, y) in self.traps:
                hit_trap = True
                break  # stepping on trap should immediately end

        # ğŸŸ¢ ä½ æ¼æ‰çš„å…³é”®ç‚¹ï¼šæ›´æ–° agent ä½ç½®
        self.agent_pos = [x, y]

        ev = {
            "found_food": found_food,
            "hit_trap": hit_trap,
            "reached_goal": reached_goal,
        }

        done = hit_trap or reached_goal
        return self.get_state(), ev, done


# ------------------- Meta-controller ---------------------


def extract_decision_direction(text: str) -> str:
    """
    åªåŒ¹é…è‹±æ–‡æ–¹å‘è¯ up/down/left/rightï¼Œ
    ä½¿ç”¨ä¸¥æ ¼å•è¯è¾¹ç•ŒåŒ¹é…ï¼Œé¿å…è¯¯åˆ¤ bright/right ç­‰æƒ…å†µã€‚
    """
    if not text:
        return "unknown"

    t = text.lower()

    # ç²¾ç¡®è‹±æ–‡åŒ¹é…ï¼ˆå¿…é¡»æ˜¯ç‹¬ç«‹å•è¯ï¼‰
    if re.search(r"\b(up|go up|move up)\b", t):
        return "up"
    if re.search(r"\b(down|go down|move down)\b", t):
        return "down"
    if re.search(r"\b(left|go left|move left)\b", t):
        return "left"
    if re.search(r"\b(right|go right|move right)\b", t):
        return "right"

    return "unknown"


STRATEGY_LOG_FILE = "strategy_evolution.jsonl"


def log_persuasion_latent(agent_name, persuasion_latent, reward, persuaded, round_id):
    entry = {
        "round": round_id,
        "agent": agent_name,
        "persuasion_latent": persuasion_latent.tolist(),
        "reward": reward,
        "persuaded": persuaded,
    }

    with open(STRATEGY_LOG_FILE, "a", encoding="utf-8") as f:
        f.write(json.dumps(entry, ensure_ascii=False))
        f.write("\n")


def meta_controller_decision(
    agent_outputs,
    trust_scores,
    emotion_agent=None,
    threshold=0.3,
    meta_memory_text=None,
):
    descriptors = {r: trust_descriptor(s) for r, s in trust_scores.items()}
    trust_summary = "\n".join(
        f"{r}: {descriptors[r]} ({trust_scores[r]:.2f})" for r in descriptors
    )
    advice_summary = "\n".join(
        (f"[{r.upper()}][{descriptors[r]}]: {o}" for r, o in agent_outputs.items())
    )
    prompt_text = (
        "You are a Meta-Controller. Choose the most reasonable move direction (up/down/left/right) from the suggestions below. "
        "Base your decision on both the suggestion content and the trust level. Output format:\n"
        "Reason for choice: XXX\nMove direction: up/down/left/right"
        f"\nCurrent trust levels:\n{trust_summary}\n\nAgent suggestions:\n{advice_summary}"
    )
    if emotion_agent and emotion_agent.mood_score < threshold:
        prompt_text += "\n Emotion agent's mood_score is low, consider prioritizing its suggestion."

    if meta_memory_text:
        prompt_text += (
            "\n\n[MetaMemory Experience]\n"
            + meta_memory_text
            + "\n(Note: this is historical reference only.)\n"
        )

    try:
        resp = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": "You are a Meta-Controller. Read suggestions and trust levels. You cannot see the map. Make the best decision.",
                },
                {"role": "user", "content": prompt_text},
            ],
            temperature=0.7,
            max_tokens=512,
        )
        reply = resp.choices[0].message.content.strip()
    except Exception as e:
        print(traceback.format_exc())
        return (
            prompt_text + f"\n[Error] Model invocation failed: {e}",
            "unknown",
            "Invocation failed",
        )

    reason, direction = "", "unknown"
    for line in reply.splitlines():
        line = line.strip()
        if line.startswith("Reason for choice") or line.startswith("é€‰æ‹©ç†ç”±"):
            if ":" in line:
                reason = line.split(":", 1)[1].strip()
            elif "ï¼š" in line:
                reason = line.split("ï¼š", 1)[1].strip()
            else:
                reason = (
                    line.replace("Reason for choice", "")
                    .replace("é€‰æ‹©ç†ç”±", "")
                    .strip()
                )
        elif line.startswith("Move direction") or line.startswith("åŠ¨ä½œæ–¹å‘"):
            if ":" in line:
                direction = extract_decision_direction(line.split(":", 1)[1])
            elif "ï¼š" in line:
                direction = extract_decision_direction(line.split("ï¼š", 1)[1])
            else:
                direction = extract_decision_direction(line)

    return prompt_text + "\n\n" + reply, direction, reason


def meta_reflect(trust_scores, outputs, chosen_action, outcome_success, reason, env):
    """
    è®© Meta åœ¨æ¯æ­¥ç»“æŸåç”Ÿæˆâ€œå…ƒåæ€â€
    è¾“å‡ºåŒ…æ‹¬ï¼š
    - æ–‡æœ¬åæ€ï¼ˆç»™ LLM ç”¨ï¼‰
    - é«˜ç»´å‘é‡ï¼ˆç”¨äºè·¨è½®è®°å¿†ï¼‰
    """
    # 1. ç”Ÿæˆåæ€æ–‡æœ¬
    trust_str = ", ".join(f"{r}:{v:.2f}" for r, v in trust_scores.items())
    agents_str = "\n".join([f"{r}: {o}" for r, o in outputs.items()])
    succ = "success" if outcome_success else "fail"

    prompt = f"""
    You are a Meta-Reflector.
    Current trust: {trust_str}
    Agents advice:
    {agents_str}

    Your decision: {chosen_action}
    Outcome: {succ}
    Reason: {reason}

    Give a short reflection on how trust should be adjusted in future.
    Then summarize 3â€“5 abstract keywords.

    Format:
    Reflection: ...
    Keywords: k1, k2, k3
    """

    text = query_openai(prompt)
    if not text:
        return None, None

    # 2. keywordsï¼ˆä½ åŸæ¥çš„é€»è¾‘ä¿æŒä¸åŠ¨ï¼‰
    lines = text.splitlines()
    keywords = []
    for ln in lines:
        if ln.strip().lower().startswith("keywords"):
            if ":" in ln:
                keywords = [x.strip() for x in ln.split(":")[1].split(",")]
            break

    # 3. ç”Ÿæˆ Meta çŠ¶æ€å‘é‡ï¼š5 trust + 1536 map + 1536 reflection_text
    trust_vec = np.array(list(trust_scores.values()), dtype=float)
    scene_vec = get_map_text_embedding(env)  # åœ°å›¾æ–‡æœ¬ embedding
    refl_vec = get_text_embedding(text)  # åæ€æ–‡æœ¬ embedding

    meta_vec = np.concatenate([trust_vec, scene_vec, refl_vec])

    return text, meta_vec


# ------------------- Main Program ---------------------
if __name__ == "__main__":
    try:
        config = ExperimentConfig()
        logger = ExperimentLogger()
        verbose = True  # Whether to print debug information

        # åˆå§‹åŒ– MetaMemory
        meta_memory = MetaMemory()
        meta_memory.load()  # åŠ è½½å·²æœ‰çš„è®°å¿†

        global_round = 0  # âœ… æ–°å¢ï¼šå…¨å±€â€œç¬¬å‡ æ­¥â€è®¡æ•°

        for episode in range(config.num_episodes):
            print(f"\n===== Episode {episode+1}/{config.num_episodes} =====")
            logger.start_episode()

            # æ¯è½®å¼€å§‹æ—¶ï¼Œæ¸…ç©ºç¼“å­˜
            meta_memory.reset_episode()

            # åœ¨ä¸»ç¨‹åºä¸­åˆå§‹åŒ– agents
            agents = {
                "emotion": Agent("emotion", bias=bias_descriptions["emotion"]["text"]),
                "rational": Agent(
                    "rational", bias=bias_descriptions["rational"]["text"]
                ),
                "habit": Agent("habit", bias=bias_descriptions["habit"]["text"]),
                "risk_monitor": Agent(
                    "risk_monitor", bias=bias_descriptions["risk_monitor"]["text"]
                ),
                "social_cognition": Agent(
                    "social_cognition",
                    bias=bias_descriptions["social_cognition"]["text"],
                ),
            }

            # åŠ è½½æ¯ä¸ªè§’è‰²çš„è®°å¿†
            for role, ag in agents.items():
                ag.reset_for_new_episode()

            env = CognitiveGridEnv(size=config.map_size)
            state = env.reset()

            trust_scores = {r: 0.4 for r in agents}

            for step in range(1, config.max_steps + 1):
                if verbose:
                    print(f"\n==== Step {step} ====")
                    env.render()

                ax, ay = state["agent"]
                gx, gy = state["goal"]
                speed = agents["emotion"].stamina
                map_str = env.render_to_string()
                situation = f"Current map:\n{map_str}\nAgent at ({ax},{ay})â†’Goal at ({gx},{gy})."

                # ---- Meta Memory Recallï¼ˆç»Ÿä¸€ä¸ºï¼š5 trust + 1536 map + 1536 textï¼‰----
                trust_vec = np.array(list(trust_scores.values()), dtype=float)

                # å½“å‰åœ°å›¾ embeddingï¼ˆå’Œ meta_reflect ç”¨çš„æ˜¯åŒä¸€ä¸ªå‡½æ•°ï¼‰
                cur_scene_vec = get_map_text_embedding(env)

                # å½“å‰æƒ…å¢ƒæ–‡æœ¬ï¼ˆä½ å¯ä»¥ç”¨ situationï¼Œä¹Ÿå¯ä»¥ç”¨åˆ«çš„ï¼Œæ¯”å¦‚ meta å†³ç­–ç†ç”±ï¼‰
                cur_text_vec = get_text_embedding(situation)

                # å½“å‰ meta çŠ¶æ€å‘é‡ï¼š5 + 1536 + 1536
                cur_meta_vec = np.concatenate([trust_vec, cur_scene_vec, cur_text_vec])

                ####
                meta_memory_text = meta_memory.get_memory_text(cur_meta_vec)
                # å¬å›æœ€ç›¸ä¼¼ episode è®°å¿†
                sim_eps = meta_memory.recall_similar_by_map(cur_scene_vec, top_k=3)

                if sim_eps:
                    sim_eps.sort(key=lambda x: x[0], reverse=True)
                    top_pairs = sim_eps[:3]  # [(sim, vec), ...]
                    top_vecs = [vec for sim, vec in top_pairs]

                    situation += "\n\n[Meta Past Experiences]\n"
                    for sim, _ in top_pairs:
                        situation += f"Similarity: {sim:.3f}\n"

                    # top_vecs ä¹‹åéšä¾¿ç”¨
                    # e.g. combined_vec = np.mean(top_vecs, axis=0)

                outputs, directions = {}, {}
                for role, ag in agents.items():
                    out = ag.respond(situation, env=env)
                    outputs[role] = out
                    directions[role] = extract_decision_direction(out)
                    if verbose:
                        print(f"[{role}] {out}")

                # è¿›è¡Œ Meta çš„å†³ç­–
                meta_decision, final_action, meta_reason = meta_controller_decision(
                    outputs,
                    trust_scores,
                    emotion_agent=agents["emotion"],
                    threshold=config.emotion_enforced_threshold,
                    meta_memory_text=meta_memory_text,
                )

                if final_action not in ["up", "down", "left", "right"]:
                    final_action = random.choice(["up", "down", "left", "right"])

                # è¾“å‡ºå†³ç­–
                if verbose:
                    print("\nMeta decision:")
                    print(meta_decision)

                prev_state = state

                # æ‰§è¡Œå†³ç­–å¹¶æ›´æ–°çŠ¶æ€
                state, ev, done = env.step(final_action, speed=speed)
                outcome_success = ev["reached_goal"] and not ev["hit_trap"]

                # è°ƒç”¨ Meta çš„åæ€æ–¹æ³•
                meta_text, meta_vec = meta_reflect(
                    trust_scores,
                    outputs,
                    final_action,
                    outcome_success,
                    meta_reason,
                    env,
                )

                if meta_vec is not None:
                    meta_memory.add_step_vector(
                        meta_vec
                    )  # å°†æ¯ä¸€æ­¥çš„åæ€å‘é‡æ·»åŠ åˆ° MetaMemory

                if verbose:
                    print(f"Action performed: {final_action}, Speed: {speed}")

                global_round += 1  # âœ… æ¯ä¸€æ­¥ç»™å›åˆæ•°+1

                # æ›´æ–°è§’è‰²å¥–åŠ±ä¸è®°å¿†
                for role, ag in agents.items():
                    persuaded = directions[role] == final_action
                    rwd = ag.evaluate_reward(ev, persuaded=persuaded)
                    ag.reflect(
                        situation,
                        rwd,
                        final_action,
                        persuaded,
                        all_outputs=outputs,
                        meta_decision=meta_decision,
                        meta_reason=meta_reason,
                        round_id=global_round,  # â† å¿…é¡»ç”¨å…³é”®å­—ä¼ 
                    )
                    if config.use_rl_learning:
                        ag.update_q_table(
                            f"{prev_state['agent'][0]},{prev_state['agent'][1]},{role}",
                            final_action,
                            f"{state['agent'][0]},{state['agent'][1]},{role}",
                        )

                    if verbose:
                        print(
                            f"{role}: Private reward {rwd:.2f}, Shared {ag.shared_reward:.2f}, Career delta {ag.last_career_delta:.2f}, Stamina {ag.stamina}"
                        )
                    # è®­ç»ƒdecoder
                    style_reward = rwd + (0.5 if persuaded else 0.0)
                    ag.update_style_decoder(style_reward)

                    # åŠ¨æ€æ›´æ–°ä¿¡ä»»
                    if config.use_dynamic_trust:
                        deltas = {}
                        for r, ag in agents.items():
                            delta = 0.1 * ag.shared_reward
                            if directions[r] == final_action:
                                delta += 0.05
                            if r == "social_cognition":
                                delta += 0.1 * max(0, ag.last_career_delta)
                            deltas[r] = delta
                        avg = sum(deltas.values()) / len(deltas)
                        for r in trust_scores:
                            raw = trust_scores[r] + deltas[r] - avg
                            trust_scores[r] = max(0.0, min(1.0, raw))

                    logger.log_step(step, agents, trust_scores, final_action)
                    if done:
                        if verbose:
                            print("Round ended: Goal reached or trap triggered.")
                        break

            logger.end_episode()
            # ä¿å­˜æ¯ä¸ªè§’è‰²çš„è®°å¿†
            meta_memory.finalize_episode()
            meta_memory.save()

            for role, ag in agents.items():
                ag.save_memory(f"{role}_memory.json")

        logger.save(config.experiment_log_file)
        logger.plot()

    except Exception:
        print("Initialization failed:", traceback.format_exc())
